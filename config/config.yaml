training: !!bool "true"
device: "cuda"
unprocessed_data: '/data5/xin/voxceleb/raw_data/dev/*/*/*.wav'
---
data:
    train_path: './train_tisv'
    #train_path_unprocessed: '/data5/eason/speakers/data/lisa/data/timit/raw/TIMIT/TRAIN/*/*/*.WAV'
    test_path: './test_tisv'
    #test_path_unprocessed: '/data5/eason/speakers/data/lisa/data/timit/raw/TIMIT/TEST/*/*/*.WAV'
    data_preprocessed: !!bool "true"
    sr: 16000
    nfft: 512 #For mel spectrogram preprocess
    window: 0.025 #(s)
    hop: 0.01 #(s)
    nmels: 40 #Number of mel energies
    tisv_frame: 180 #Max number of time steps in input after preprocess
---
model:
    hidden: 768 #Number of LSTM hidden layer units
    num_layer: 3 #Number of LSTM layers
    proj: 256 #Embedding size
    model_path: 'ckpt/600_person_20_uter_epoch_120.pth' #Model path for testing, inference, or resuming training
---
train:
    N : 64 #Number of speakers in batch
    M : 10 #Number of utterances per speaker
    num_workers: 8 #number of workers for data laoder
    lr: 0.01
    epochs: 950 #Max training speaker epoch 
    log_interval: 1 #Epochs before printing progress
    log_file: '/data5/eason/court/logs/log.txt'
    checkpoint_interval: 5 #Save model after x speaker epochs
    checkpoint_dir: '/data5/eason/court/ckpt/64_batch_size/'
    restore: !!bool "false" #Resume training from previous model path
---
test:
    N : 4 #Number of speakers in batch
    M : 6 #Number of utterances per speaker
    num_workers: 8 #number of workers for data laoder
    epochs: 10 #testing speaker epochs
